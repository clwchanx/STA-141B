{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b8fb8c6",
   "metadata": {},
   "source": [
    "# Web Scraper (Protoptype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827806c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup \n",
    "import nltk\n",
    "import numpy as np\n",
    "import re\n",
    "from nltk.corpus import wordnet\n",
    "from selenium import webdriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5c74f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def raw_text(url):\n",
    "    \"\"\"Takes a URL as input and performs web scrapping to retrieve the body of the\n",
    "    webpage (in this case a Linkedin ad)\"\"\"\n",
    "    ad = requests.get(url) #Retrieve webpage\n",
    "    Html = BeautifulSoup(ad.text, 'html.parser') #Convert html into a nicer format\n",
    "    text_body = Html.find_all('div', \n",
    "                              {'class':\"show-more-less-html__markup show-more-less-html__markup--clamp-after-5\"})\n",
    "    text_body = text_body[0].text\n",
    "    return text_body\n",
    "def clean_text(doc):\n",
    "    \"\"\"Take an unstructured document and tokenize it into a list of words. \n",
    "    Then standardize it by lowercasing and lemmatizing each word\"\"\"\n",
    "    words = re.findall(r'(?:[a-zA-Z]|#|\"+\")+',doc) #Find all alphabetical words (Preserve + and # for C++ and C#)\n",
    "    clean = [i for i in words if i.isupper() or i.islower()] #Retrieve all words that aren't glued to each other\n",
    "    dirty = [i for i in words if not i.islower() and not i.isupper()] #Retrieve words stuck together\n",
    "    dirty = [re.findall('[a-zA-Z][^A-Z]*',i) for i in dirty] #Split all the tangled words ie split 'ThisExample' into ['This','Example']\n",
    "    clean2 = [j for i in dirty for j in i] #Unlist the list of lists\n",
    "    words = clean + clean2 #Combine all the words together\n",
    "    stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "    words = [i.lower() for i in words] #Lowercase all words\n",
    "    words = [i for i in words if i not in stopwords] #Filter out stopwords\n",
    "    tag_words = nltk.pos_tag(words) #Begin lemmatizing by tagging each word\n",
    "    tag_words = [(i, wordnet_pos(j)) for (i, j) in tag_words] #Convert the tags into something the lemmatizer understands\n",
    "    lemmatizer = nltk.WordNetLemmatizer()\n",
    "    clean_words = [lemmatizer.lemmatize(i, j) for i, j in tag_words] #Lemmatize the words\n",
    "    #Document should be cleaned up\n",
    "    return clean_words\n",
    "def wordnet_pos(tag):\n",
    "    \"\"\"Map a Brown POS tag to a WordNet POS tag.\"\"\"\n",
    "    \n",
    "    table = {\"N\": wordnet.NOUN, \"V\": wordnet.VERB, \"R\": wordnet.ADV, \"J\": wordnet.ADJ}\n",
    "    \n",
    "    # Default to a noun.\n",
    "    return table.get(tag[0], wordnet.NOUN) #Function created by Bo Ning in Week 6-2\n",
    "def lang_count(TXT):\n",
    "    \"\"\" Take a body of clean text and count the number of programming languages present\"\"\"\n",
    "    languages = ['python','r','sql','sa','c',\n",
    "                 'c++','c#','java','javascript',\n",
    "                 'julia','matlab','swift','tableau'\n",
    "                'microsoft','github'] #SAS turns into sa after lemmatization\n",
    "    #ADD MORE LANGUAGES IF NECESSARY\n",
    "    count = sum([i in TXT for i in languages]) #Check if each language is in the ad\n",
    "    #And sum the number of programming languages present\n",
    "    return count\n",
    "def get_salary(TXT):\n",
    "    \"\"\"From a body of raw text, retrieve the salary\"\"\"\n",
    "    salaries = re.findall(r\"(\\$\\d+\\,\\d+\\.\\d{1,2})\",TXT) \n",
    "    if salaries != []:\n",
    "        return salaries[-1] #Let's work with the maximum salary\n",
    "    else:\n",
    "        salaries = re.findall(r\"(\\$\\d+\\,\\d+)\",TXT) \n",
    "        if salaries != []:\n",
    "            return salaries[-1]\n",
    "        else:\n",
    "            return \"NaN\"\n",
    "def ML_skill(TXT):\n",
    "    \"\"\"Using a body of clean text, check whether the words machine learning is present\n",
    "    to see if it is a required skill\"\"\"\n",
    "    return str(int('machine' in TXT and 'learning' in TXT))\n",
    "def get_edu(TXT):\n",
    "    \"\"\"Using a body of raw text, retrieve the education level\"\"\"\n",
    "    if \"Master\" in TXT and \"Bachelor\" in TXT:\n",
    "        return \"4\" #Category where Bachlor's is minimum but higher level preferred\n",
    "    elif \"PhD\" in TXT:\n",
    "        return \"3\"\n",
    "    elif \"Master\" in TXT:\n",
    "        return \"2\"\n",
    "    elif \"Bachelor\" in TXT:\n",
    "        return \"1\"\n",
    "    else:\n",
    "        return \"0\" #No education specified\n",
    "def ad_type(TXT):\n",
    "    \"\"\"Using a body of clean text, check whether this is an ad for data analyst or scientists\"\"\"\n",
    "    return str(int(\"science\" in TXT))\n",
    "def benefits(TXT):\n",
    "    \"\"\"Using a body of raw text, check if benefits are included\"\"\"\n",
    "    return str(int('Benefits' in TXT or 'benefits' in TXT))\n",
    "def exp(TXT):\n",
    "    \"\"\"Using a body of raw text, check if experience is required/preferred\"\"\"\n",
    "    sentences = nltk.sent_tokenize(TXT) #Split text into sentences\n",
    "    years = [re.findall(r\"\\d+.*year\", i) for i in sentences] #Find sentences with years in it\n",
    "    for items in years:\n",
    "        if items != []:\n",
    "            years = [i for i in years if i != []][0][0] #Get rid of empty values and turn the years of experience into a string\n",
    "            year = re.findall(r'\\d+',years)[0]\n",
    "            return year\n",
    "    return \"NaN\"\n",
    "def collect_data(url):\n",
    "    \"\"\"Input a URL for a Linkedin Ad and retrieve all relevant data\"\"\"\n",
    "    raw = raw_text(url)\n",
    "    clean = clean_text(raw)\n",
    "    return {'Languages':lang_count(clean),\n",
    "            'Salary':get_salary(raw),\n",
    "            'Machine Learning':ML_skill(clean),\n",
    "            'Education':get_edu(raw),\n",
    "            'Type': ad_type(clean),\n",
    "            'Benefits':benefits(raw),\n",
    "            'Experience':exp(raw),\n",
    "            'url':url}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
