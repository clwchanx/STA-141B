{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b8fb8c6",
   "metadata": {},
   "source": [
    "# Web Scraper (Protoptype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827806c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup \n",
    "import nltk\n",
    "import numpy as np\n",
    "import re\n",
    "from nltk.corpus import wordnet\n",
    "from selenium import webdriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5c74f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def raw_text(url):\n",
    "    \"\"\"Takes a URL as input and performs web scrapping to retrieve the body of the\n",
    "    webpage (in this case a Linkedin ad)\"\"\"\n",
    "    ad = requests.get(url) #Retrieve webpage\n",
    "    Html = BeautifulSoup(ad.text, 'html.parser') #Convert html into a nicer format\n",
    "    text_body = Html.find_all('div', \n",
    "                              {'class':\"show-more-less-html__markup show-more-less-html__markup--clamp-after-5\"})\n",
    "    text_body = text_body[0].text\n",
    "    return text_body\n",
    "def clean_text(doc):\n",
    "    \"\"\"Take an unstructured document and tokenize it into a list of words. \n",
    "    Then standardize it by lowercasing and lemmatizing each word\"\"\"\n",
    "    words = re.findall(r'(?:[a-zA-Z]|#|\"+\")+',doc) #Find all alphabetical words (Preserve + and # for C++ and C#)\n",
    "    stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "    words = [i.lower() for i in words] #Lowercase all words\n",
    "    words = [i for i in words if i not in stopwords] #Filter out stopwords\n",
    "    tag_words = nltk.pos_tag(words) #Begin lemmatizing by tagging each word\n",
    "    tag_words = [(i, wordnet_pos(j)) for (i, j) in tag_words] #Convert the tags into something the lemmatizer understands\n",
    "    lemmatizer = nltk.WordNetLemmatizer()\n",
    "    clean_words = [lemmatizer.lemmatize(i, j) for i, j in tag_words] #Lemmatize the words\n",
    "    #Document should be cleaned up\n",
    "    return clean_words\n",
    "def wordnet_pos(tag):\n",
    "    \"\"\"Map a Brown POS tag to a WordNet POS tag.\"\"\"\n",
    "    \n",
    "    table = {\"N\": wordnet.NOUN, \"V\": wordnet.VERB, \"R\": wordnet.ADV, \"J\": wordnet.ADJ}\n",
    "    \n",
    "    # Default to a noun.\n",
    "    return table.get(tag[0], wordnet.NOUN) #Function created by Bo Ning inWeek 6-2\n",
    "def lang_count(text):\n",
    "    \"\"\" Take a body of clean text and count the number of programming languages present\"\"\"\n",
    "    languages = ['python','r','sql','sa','c',\n",
    "                 'c++','c#','java','javascript',\n",
    "                 'julia','matlab','swift','tableau'\n",
    "                'microsoft'] #SAS turns into sa after lemmatization\n",
    "    count = sum([i in text for i in languages]) #Check if each language is in the ad\n",
    "    #And sum the number of programming languages present\n",
    "    return count\n",
    "def get_salary(text):\n",
    "    \"\"\"From a body of raw text, retrieve the salary\"\"\"\n",
    "    salaries = re.findall(r\"(\\d+\\,\\d+\\.\\d{1,2})\",text) \n",
    "    if salaries != []:\n",
    "        return salaries[-1] #Let's work with the maximum salary\n",
    "    else:\n",
    "        salaries = re.findall(r\"(\\d+\\,\\d+)\",text) \n",
    "        if salaries != []:\n",
    "            return salaries[-1]\n",
    "        else:\n",
    "            return \"NaN\"\n",
    "def ML_skill(text):\n",
    "    \"\"\"Using a body of clean text, check whether the words machine learning is present\n",
    "    to see if it is a required skill\"\"\"\n",
    "    return str(int('machine' in text and 'learning' in text))\n",
    "def get_edu(text):\n",
    "    \"\"\"Using a body of raw text, retrieve the education level\"\"\"\n",
    "    if \"Master\" in text and \"Bachelor\" in text:\n",
    "        return \"4\" #Category where Bachlor's is minimum but higher level preferred\n",
    "    elif \"PhD\" in text:\n",
    "        return \"3\"\n",
    "    elif \"Master\" in text:\n",
    "        return \"2\"\n",
    "    elif \"Bachelor\" in text:\n",
    "        return \"1\"\n",
    "    else:\n",
    "        return \"0\" #No education specified\n",
    "def ad_type(text):\n",
    "    \"\"\"Using a body of clean text, check whether this is an ad for data analyst or scientists\"\"\"\n",
    "    return str(int(\"science\" in text))\n",
    "def benefits(text):\n",
    "    \"\"\"Using a body of raw text, check if benefits are included\"\"\"\n",
    "    return str(int('Benefits' in text or 'benefits' in text))\n",
    "def exp(text):\n",
    "    \"\"\"Using a body of raw text, check if experience is required/preferred\"\"\"\n",
    "    return str(int(\"experience\" in text))\n",
    "def collect_data(url):\n",
    "    \"\"\"Input a URL for a Linkedin Ad and retrieve all relevant data\"\"\"\n",
    "    raw = raw_text(url)\n",
    "    clean = clean_text(raw)\n",
    "    return {'Languages':lang_count(clean),\n",
    "            'Salary':get_salary(raw),\n",
    "            'Machine Learning':ML_skill(clean),\n",
    "            'Education':get_edu(raw),\n",
    "            'Type': ad_type(clean),\n",
    "            'Benefits':benefits(raw),\n",
    "            'Experience':exp(raw)}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
