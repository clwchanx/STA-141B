{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b8fb8c6",
   "metadata": {},
   "source": [
    "# Web Scraper (Prototype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5c74f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup \n",
    "import nltk\n",
    "import numpy as np\n",
    "import re\n",
    "from nltk.corpus import wordnet\n",
    "from selenium import webdriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0c586a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def raw_text(url):\n",
    "    \"\"\"Takes a URL as input and performs web scrapping to retrieve the body of the\n",
    "    webpage (in this case a Linkedin ad)\"\"\"\n",
    "    ad = requests.get(url) #Retrieve webpage\n",
    "    Html = BeautifulSoup(ad.text, 'html.parser') #Convert html into a nicer format\n",
    "    text_body = Html.find_all('div', \n",
    "                              {'class':\"show-more-less-html__markup show-more-less-html__markup--clamp-after-5\"})\n",
    "    text_body = text_body[0].text\n",
    "    return text_body\n",
    "def clean_text(doc):\n",
    "    \"\"\"Take an unstructured document and tokenize it into a list of words. \n",
    "    Then standardize it by lowercasing and lemmatizing each word\"\"\"\n",
    "    words = re.findall(r'(?:[a-zA-Z]|#|\"+\")+',doc) #Find all alphabetical words (Preserve + and # for C++ and C#)\n",
    "    stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "    words = [i.lower() for i in words] #Lowercase all words\n",
    "    words = [i for i in words if i not in stopwords] #Filter out stopwords\n",
    "    tag_words = nltk.pos_tag(words) #Begin lemmatizing by tagging each word\n",
    "    tag_words = [(i, wordnet_pos(j)) for (i, j) in tag_words] #Convert the tags into something the lemmatizer understands\n",
    "    lemmatizer = nltk.WordNetLemmatizer()\n",
    "    clean_words = [lemmatizer.lemmatize(i, j) for i, j in tag_words] #Lemmatize the words\n",
    "    #Document should be cleaned up\n",
    "    return clean_words\n",
    "def wordnet_pos(tag):\n",
    "    \"\"\"Map a Brown POS tag to a WordNet POS tag.\"\"\"\n",
    "    \n",
    "    table = {\"N\": wordnet.NOUN, \"V\": wordnet.VERB, \"R\": wordnet.ADV, \"J\": wordnet.ADJ}\n",
    "    \n",
    "    # Default to a noun.\n",
    "    return table.get(tag[0], wordnet.NOUN) #Function created by Bo Ning inWeek 6-2\n",
    "def lang_count(text):\n",
    "    \"\"\" Take a body of clean text and count the number of programming languages present\"\"\"\n",
    "    languages = ['python','r','sql','sa','c',\n",
    "                 'c++','c#','java','javascript',\n",
    "                 'julia','matlab','swift','tableau'\n",
    "                'microsoft'] #SAS turns into sa after lemmatization\n",
    "    count = sum([i in text for i in languages]) #Check if each language is in the ad\n",
    "    #And sum the number of programming languages present\n",
    "    return count"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
